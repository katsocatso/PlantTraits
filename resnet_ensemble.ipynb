{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "! pip install --quiet \"ipython[notebook]==7.34.0, <8.17.0\" \"setuptools>=68.0.0, <68.3.0\" \"tensorboard\" \"lightning>=2.0.0\" \"urllib3\" \"torch==2.3.1\" \"matplotlib\" \"pytorch-lightning>=1.4, <2.1.0\" \"seaborn\" \"torchvision\" \"numpy\" \"pandas\" \"tensorflow\" \"scikit-learn\" \"torchmetrics>=0.7, <1.3\" \"matplotlib>=3.0.0, <3.9.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Standard Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "# Setting the seed\n",
    "L.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "if device == torch.device(\"cuda:0\"):\n",
    "  print('Using GPU')\n",
    "else:\n",
    "  print('GPU is not detected.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data from Kaggle\n",
    "This is only needed if you're running on Google Colab. If you're running this notebook locally, ensure that the data is downloaded and in the same folder (i.e. `train.csv`, `test.csv`, `target_name_meta.tsv`, `test_images` and `train_images` should be in the current directory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to Kaggle via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --quiet kaggle\n",
    "!echo '{\"username\":\"katsocatso\",\"key\":\"6bee42e5aa15cc4b7efa6106cc128fa7\"}' > kaggle.json\n",
    "!mkdir -p /root/.kaggle\n",
    "!mv kaggle.json /root/.kaggle\n",
    "!chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!kaggle competitions download -c cs-480-2024-spring\n",
    "!unzip -qq *.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data into dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "\n",
    "def log10_with_neg(x):\n",
    "  if x == 0:\n",
    "    return 0\n",
    "  elif x < 0:\n",
    "    return -math.log10(-x)\n",
    "  else:\n",
    "    return math.log10(x)\n",
    "\n",
    "\n",
    "def inverse_log10_with_neg(x):\n",
    "  if x == 0:\n",
    "    return 0\n",
    "  elif x < 0:\n",
    "    return -(10 ** -x)\n",
    "  else:\n",
    "    return 10 ** x\n",
    "\n",
    "inverse_log10_with_neg_all = np.vectorize(inverse_log10_with_neg)\n",
    "\n",
    "def preprocess_log_all_targets(df, log_func):\n",
    "  target_cols = df.columns[-num_targets:]\n",
    "  for col in target_cols:\n",
    "    df[col] = df[col].map(log_func)\n",
    "\n",
    "def remove_ids_from_df(df):\n",
    "  ids = df['id'].values\n",
    "  df = df.iloc[:, 1:]\n",
    "  return ids, df\n",
    "\n",
    "\n",
    "def normalize(df):\n",
    "  scaler = MinMaxScaler()\n",
    "  normalized_df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "  return scaler, normalized_df\n",
    "\n",
    "\n",
    "def remove_outliers(df):\n",
    "  target_cols = df.columns[-num_targets:]\n",
    "  for col in target_cols:\n",
    "    mean = df[col].mean()\n",
    "    std = df[col].std()\n",
    "    df = df[(df[col] >= mean - 3 * std) & (df[col] <= mean + 3 * std)]\n",
    "  return df\n",
    "\n",
    "\n",
    "num_input_traits = 163\n",
    "num_targets = 6\n",
    "\n",
    "# Uncomment when running in Colab\n",
    "# train_image_path = 'data/train_images'\n",
    "# train_csv_file = 'data/train.csv'\n",
    "\n",
    "# test_image_path = 'data/test_images'\n",
    "# test_csv_file = 'data/test.csv'\n",
    "\n",
    "# Uncomment when running locally\n",
    "train_image_path = './train_images'\n",
    "train_csv_file = './train.csv'\n",
    "\n",
    "test_image_path = './test_images'\n",
    "test_csv_file = './test.csv'\n",
    "\n",
    "df_train_and_val = pd.read_csv(train_csv_file)\n",
    "df_test = pd.read_csv(test_csv_file)\n",
    "\n",
    "# Split df_train into train and validation set (ratio 4:1)\n",
    "df_val = df_train_and_val.sample(frac = 0.25)\n",
    "df_train = df_train_and_val.drop(df_val.index)\n",
    "\n",
    "# Get the ids and then remove from the df\n",
    "ids_train, df_train = remove_ids_from_df(df_train)\n",
    "ids_val, df_val = remove_ids_from_df(df_val)\n",
    "ids_test, df_test = remove_ids_from_df(df_test)\n",
    "\n",
    "# Normalize\n",
    "scaler_train, normalized_df_train = normalize(df_train)\n",
    "scaler_val, normalized_df_val = normalize(df_val)\n",
    "scaler_test, normalized_df_test = normalize(df_test)\n",
    "\n",
    "# targets_df=pd.read_csv('data/target_name_meta.tsv', sep='\\t')\n",
    "targets_df=pd.read_csv('./target_name_meta.tsv', sep='\\t')\n",
    "target_ids = targets_df['trait_ID']\n",
    "target_indices_train = [df_train.columns.get_loc(trait + \"_mean\") for trait in target_ids]\n",
    "print(\"The traits we are trying to predict are:\")\n",
    "print(targets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_folder, df, ids, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.df = df\n",
    "        self.ids = ids\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        id = self.ids[idx]\n",
    "        image = Image.open(self.image_folder + f\"/{id}.jpeg\")\n",
    "\n",
    "        input_traits = self.df.iloc[idx, :num_input_traits].values.astype('float')\n",
    "        targets = self.df.iloc[idx, num_input_traits:].values.astype('float')\n",
    "\n",
    "        image = self.transform(image)\n",
    "        # image_batch = image.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "        # print(image_batch.shape)\n",
    "        \n",
    "        return image, torch.tensor(input_traits, dtype=torch.float), torch.tensor(targets, dtype=torch.float), id\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "# input_tensor = preprocess(input_image)\n",
    "# input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "\n",
    "# Create the dataset\n",
    "train_dataset = ImageDataset(train_image_path, normalized_df_train, ids_train, transform)\n",
    "val_dataset = ImageDataset(train_image_path, normalized_df_val, ids_train, transform)\n",
    "test_dataset = ImageDataset(test_image_path, normalized_df_test, ids_test, transform)\n",
    "\n",
    "batch_size = 20\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_custom_resnet():\n",
    "    resnet = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "    resnet.fc = nn.Identity()\n",
    "    return resnet\n",
    "\n",
    "\n",
    "class SpecializedModel(nn.Module):\n",
    "    def __init__(self, resnet, num_aux_vars, hidden_size, output_size):\n",
    "        super(SpecializedModel, self).__init__()\n",
    "        self.resnet = resnet\n",
    "        self.image_fc = nn.Linear(512, hidden_size)\n",
    "        \n",
    "        self.trait_fc1 = nn.Linear(num_aux_vars, hidden_size)\n",
    "        self.trait_fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_size + hidden_size // 2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.fc3 = nn.Linear(hidden_size // 2, output_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        # self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "\n",
    "    def forward(self, image, input_traits):\n",
    "        # Image layers\n",
    "        image_features = self.resnet(image)\n",
    "        image_features = self.relu(self.image_fc(image_features))\n",
    "        \n",
    "        # Auxiliary variable layers\n",
    "        trait_features = self.relu(self.trait_fc1(input_traits.float()))\n",
    "        trait_features = self.relu(self.trait_fc2(trait_features))\n",
    "        \n",
    "        # Concatenate image + auxiliary\n",
    "        combined_input = torch.cat((image_features, trait_features), dim=1)\n",
    "        fc1 = self.relu(self.fc1(combined_input))\n",
    "        fc2 = self.relu(self.fc2(fc1))\n",
    "        output = self.fc3(fc2)\n",
    "        return output\n",
    "\n",
    "\n",
    "class ResNetExtendedModel(nn.Module):\n",
    "    def __init__(self, auxiliary_num_vars, num_targets, auxiliary_idx):\n",
    "        super(ResNetExtendedModel, self).__init__()\n",
    "        self.worldclim = SpecializedModel(get_custom_resnet(), auxiliary_num_vars['WORLDCLIM'], 64, 64)\n",
    "        self.soil = SpecializedModel(get_custom_resnet(), auxiliary_num_vars['SOIL'], 64, 64)\n",
    "        self.modis = SpecializedModel(get_custom_resnet(), auxiliary_num_vars['MODIS'], 64, 64)\n",
    "        # self.vod = SpecializedModel(get_custom_resnet(), auxiliary_num_vars['VOD'], 64, 64)\n",
    "        \n",
    "        combined_input_size = 64 * 3  # outputs from each specialized model\n",
    "        self.fc1 = nn.Linear(combined_input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_targets)\n",
    "        self.relu = nn.ReLU()\n",
    "        # self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        # Save the indices of each category of auxiliary vars\n",
    "        self.auxiliary_idx = auxiliary_idx\n",
    "\n",
    "    def forward(self, image, input_traits):\n",
    "        # Process through each category-specific network\n",
    "        worldclim_start, worldclim_end = self.auxiliary_idx['WORLDCLIM'][0], self.auxiliary_idx['WORLDCLIM'][1]\n",
    "        worldclim_output = self.worldclim(image, input_traits[:, worldclim_start:worldclim_end])\n",
    "\n",
    "        soil_start, soil_end = self.auxiliary_idx['SOIL'][0], self.auxiliary_idx['SOIL'][1]\n",
    "        soil_output = self.soil(image, input_traits[:, soil_start:soil_end])\n",
    "\n",
    "        modis_start, modis_end = self.auxiliary_idx['MODIS'][0], self.auxiliary_idx['MODIS'][1]\n",
    "        modis_output = self.modis(image, input_traits[:, modis_start:modis_end])\n",
    "\n",
    "        # vod_start, vod_end = self.auxiliary_idx['VOD'][0], self.auxiliary_idx['VOD'][1]\n",
    "        # vod_output = self.vod(image, input_traits[:, vod_start:vod_end])\n",
    "\n",
    "        # Concatenate features from all category-specific networks\n",
    "        combined_input = torch.cat((worldclim_output, soil_output, modis_output), dim=1)\n",
    "\n",
    "        # Final fully connected layers\n",
    "        output = self.relu(self.fc1(combined_input))\n",
    "        output = self.relu(self.fc2(output))\n",
    "        # x = self.dropout(x)\n",
    "        output = self.fc3(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "\n",
    "    i = 0\n",
    "    for images, input_traits, targets, _ in dataloader:\n",
    "        images = images.to(device)\n",
    "        input_traits = input_traits.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        if i % 50 == 0 and i != 0:\n",
    "            print(f\"Trained {i} batches so far...\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model.forward(images, input_traits)\n",
    "        # pred = pred.cpu()\n",
    "\n",
    "        loss = criterion(pred, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        i += 1\n",
    "    return running_loss\n",
    "\n",
    "\n",
    "def validate(dataloader, model, criterion, scaler):\n",
    "    model.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for images, input_traits, targets, _ in dataloader:\n",
    "            images = images.to(device)\n",
    "            input_traits = input_traits.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            if i % 50 == 0 and i != 0:\n",
    "                print(f\"Tested {i} batches so far...\")\n",
    "            pred = model.forward(images, input_traits)\n",
    "\n",
    "            # Move stuff to CPU first so its compatible with the .to('cuda') stuff\n",
    "            pred_cpu = pred.cpu().numpy()\n",
    "            targets_cpu = targets.cpu().numpy()\n",
    "\n",
    "            # Append pred to 163 cols of zeroes, then denormalize\n",
    "            # We convert from numpy <-> Tensor because inverse_transform only takes np.array\n",
    "            zeroes = np.zeros((pred_cpu.shape[0], num_input_traits))\n",
    "            denormalized_pred = scaler.inverse_transform(np.hstack([zeroes, pred_cpu]))[:, -num_targets]\n",
    "            denormalized_pred = torch.from_numpy(denormalized_pred)\n",
    "            # denormalized_pred = torch.from_numpy(inverse_log10_with_neg_all(denormalized_pred))\n",
    "            \n",
    "            # Do the same thing for targets, then compare\n",
    "            denormalized_targets = scaler.inverse_transform(np.hstack([zeroes, targets_cpu]))[:, -num_targets]\n",
    "            denormalized_targets = torch.from_numpy(denormalized_targets)\n",
    "            # denormalized_targets = torch.from_numpy(inverse_log10_with_neg_all(denormalized_targets))\n",
    "\n",
    "            test_loss += criterion(denormalized_pred, denormalized_targets).item()\n",
    "            i += 1\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the start and end index of each category in the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_category = {\n",
    "    0: 'WORLDCLIM',\n",
    "    1: 'SOIL',\n",
    "    2: 'MODIS',\n",
    "    3: 'VOD'\n",
    "}\n",
    "\n",
    "auxiliary_idx = {\n",
    "    'WORLDCLIM': [],\n",
    "    'SOIL': [],\n",
    "    'MODIS': [],\n",
    "    'VOD': []\n",
    "}\n",
    "\n",
    "curr_id = 0\n",
    "for idx, col in enumerate(df_train.columns[:num_input_traits]):\n",
    "    curr_category = id_to_category[curr_id]\n",
    "\n",
    "    if not auxiliary_idx[curr_category]:\n",
    "        # To start the first category\n",
    "        auxiliary_idx[curr_category].append(idx)\n",
    "        \n",
    "    if curr_category not in col:\n",
    "        auxiliary_idx[curr_category].append(idx)\n",
    "        curr_id += 1\n",
    "        # Starting the next category\n",
    "        auxiliary_idx[id_to_category[curr_id]].append(idx)\n",
    "\n",
    "# Add the ending idx of the last category\n",
    "auxiliary_idx[id_to_category[curr_id]].append(idx + 1)\n",
    "\n",
    "print(auxiliary_idx)\n",
    "\n",
    "auxiliary_num_vars = {\n",
    "    'WORLDCLIM': 0,\n",
    "    'SOIL': 0,\n",
    "    'MODIS': 0,\n",
    "    'VOD': 0\n",
    "}\n",
    "# Verifying that the aux indices are correct and also initialize auxiliary_num_vars\n",
    "for id in id_to_category:\n",
    "    category = id_to_category[id]\n",
    "    aux_id = auxiliary_idx[category]\n",
    "    auxiliary_num_vars[category] = aux_id[1] - aux_id[0]\n",
    "\n",
    "    cols = df_train.columns[aux_id[0]: aux_id[1]].tolist()\n",
    "    print(id_to_category[id], len(cols))\n",
    "    print(aux_id[1] - aux_id[0])\n",
    "    print(cols)\n",
    "print(auxiliary_num_vars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now actually do the training and validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import sys\n",
    "\n",
    "model = ResNetExtendedModel(auxiliary_num_vars, num_targets, auxiliary_idx)\n",
    "\n",
    "# best_model = copy.deepcopy(model.state_dict())\n",
    "model.to('cuda')\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "test_loss = validate(val_loader, model, criterion, scaler_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import sys\n",
    "\n",
    "# Freeze layers\n",
    "# for param in model.parameters():\n",
    "#      param.requires_grad = False\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     if \"layer4\" in name:\n",
    "#         param.requires_grad = True\n",
    "\n",
    "model = ResNetExtendedModel(auxiliary_num_vars, num_targets, auxiliary_idx)\n",
    "best_model = copy.deepcopy(model.state_dict())\n",
    "model.to('cuda')\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# train_accuracy_metrics = []\n",
    "train_loss_metrics = []\n",
    "# test_accuracy_metrics = []\n",
    "test_loss_metrics = []\n",
    "\n",
    "# Training the model\n",
    "\n",
    "num_epochs = 5\n",
    "epochs_lst = range(1, num_epochs + 1)\n",
    "best_loss = 9223372036854775807\n",
    "\n",
    "# Implementing early stopping\n",
    "# patience = 3\n",
    "# patience_ctr = 0\n",
    "# tolerance = 0.01\n",
    "\n",
    "for epoch in epochs_lst:\n",
    "    print(f\"Starting epoch {epoch}\")\n",
    "    train_accuracy = 0\n",
    "\n",
    "    # Training\n",
    "    train_loss = train(train_loader, model, criterion, optimizer, device)\n",
    "    train_loss_metrics.append(train_loss)\n",
    "\n",
    "    # Testing on validation set\n",
    "    test_loss = validate(val_loader, model, criterion, scaler_train)\n",
    "    test_loss_metrics.append(test_loss)\n",
    "    # test_accuracy_metrics.append(test_accuracy)\n",
    "\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        # patience_ctr = 0\n",
    "        best_model = copy.deepcopy(model.state_dict())\n",
    "    # else:\n",
    "    #     patience += 1\n",
    "    \n",
    "    # if patience_ctr >= patience:\n",
    "    #     break\n",
    "\n",
    "    print(f\"test_loss = {test_loss} for epoch {epoch}\")\n",
    "    print(f\"Done epoch {epoch}!\")\n",
    "print(\"Done training and testing!\")\n",
    "model.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Testing loss\")\n",
    "print(test_loss_metrics)\n",
    "print()\n",
    "\n",
    "print(\"Training loss\")\n",
    "print(train_loss_metrics)\n",
    "print()\n",
    "\n",
    "plt.plot(epochs_lst, test_loss_metrics)\n",
    "plt.title(\"Test loss vs Epochs\")\n",
    "plt.ylabel(\"Test loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions for Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict(model, dataloader, scaler):\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    ids = []\n",
    "\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for images, input_traits, _, id in dataloader:\n",
    "            images = images.to(device)\n",
    "            input_traits = input_traits.to(device)\n",
    "            # input_traits = input_traits.to(device)\n",
    "            \n",
    "            if i % 50 == 0 and i != 0:\n",
    "                print(f\"Tested {i} batches so far...\")\n",
    "            pred = model(images, input_traits)\n",
    "            pred = pred.cpu().numpy()\n",
    "            # pred = inverse_log10_with_neg_all(pred)\n",
    "            predictions.append(pred)\n",
    "            ids.extend(id.numpy())\n",
    "\n",
    "            i += 1\n",
    "    print(\"Done testing!\")\n",
    "\n",
    "    return ids, predictions\n",
    "\n",
    "ids, predictions = predict(model, test_loader, scaler_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ids))\n",
    "predictions = np.vstack(predictions)\n",
    "\n",
    "scaled_test_data = normalized_df_test.to_numpy()\n",
    "\n",
    "# Concatenate original normalized test data with predictions and denormalize\n",
    "normalized_test_data_with_pred = np.hstack([scaled_test_data, predictions])\n",
    "denormalized_predictions = scaler_train.inverse_transform(normalized_test_data_with_pred)[:, -num_targets:]\n",
    "\n",
    "submission_df = pd.DataFrame(denormalized_predictions, columns=[\n",
    "    'X4','X11','X18','X26','X50','X3112'\n",
    "])\n",
    "submission_df.insert(0, 'id', ids)\n",
    "submission_df.to_csv('submission_resnet_ensemble_3epoch.csv', index=False)\n",
    "print(\"Predictions saved to submission_resnet.csv!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
